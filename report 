1 | P a g e
PROBLEM STATEMENT 6
Product Category Creation for Healthcare Kiosks in India
Category: Artificial Intelligence, Machine Learning, Computer Vision, Federated
Learning, Healthcare, Electronic Health Records (EHRs), Telemedicine.
We choose Category: Machine Learning
AIM:
We are planning to develop a machine learning model that will remind patients, their family members, or children to take their medicine on time using text-to-speech and vice versa. It should also send notifications to the patient. This will be especially helpful when children or family members are living abroad, as they can still monitor and remind the patient remotely. We are using PyTorch, and the medicine reminders will work through both speech-to-text and text-to-speech. The patient's name is Ravi, and he must take his medicine three times a day as prescribed by the diagnostic machine ‚Äî at 9:00 AM (morning), 2:00 PM (afternoon), and 8:00 PM (night).
Prework:
As a prerequisite for this project, we went through several YouTube playlists. To begin this project, we completed some preliminary work, including studying the relevant YouTube playlists.
1)https://youtube.com/playlist?list=PLwATfeyAMNoirN4idjev6aRu8ISZYVWm&si=2i84bQ4FsYOy7Xa-
2)https://youtube.com/playlist?list=PLwATfeyAMNqIee7cH3q1bh4QJFAaeNv0&si=2048i5cuyV1RUpS8
The summary of the pre-work playlist is as follows:
Overview:
This guide is an intermediate-level introduction to audio signal processing, emphasizing its application in machine learning and AI, especially for tasks like speech recognition, music analysis, and sound classification. It blends core signal processing concepts with practical Python implementations using Librosa and NumPy.
Key Applications:
ÔÇ∑
Speech recognition (ASR)
ÔÇ∑
Speaker verification
ÔÇ∑
Audio denoising
ÔÇ∑
Music information retrieval (genre detection, instrument identification, mood analysis)
2 | P a g e
Target Audience:
ÔÇ∑
ÔÇ∑ ML engineers, software developers, CS students
ÔÇ∑
ÔÇ∑ Musicians & audio tech enthusiasts
ÔÇ∑
ÔÇ∑ Assumes prior knowledge of ML or DSP basics
Core Concepts Covered:
1. Fundamentals of Sound:
ÔÇ∑
ÔÇ∑ Sound is a mechanical longitudinal wave made of compression and rarefaction regions.
ÔÇ∑
ÔÇ∑ Frequency (Hz) controls pitch, amplitude controls loudness.
ÔÇ∑
ÔÇ∑ Waveform = pressure variation over time; analyzed for audio characteristics.
ÔÇ∑
ÔÇ∑ Sounds are either periodic (like sine waves) or aperiodic (like noise).
2. Human Hearing & Perception:
ÔÇ∑
ÔÇ∑ Hearing range: ~20 Hz to 20 kHz
ÔÇ∑
ÔÇ∑ Pitch is perceived logarithmically; relates to octaves and MIDI notes
ÔÇ∑
ÔÇ∑ Timbre differentiates sounds with same pitch/loudness ‚Äî shaped by:
o Harmonics
o ADSR envelope
o Modulations (vibrato/tremolo)
3. Digital Audio Basics:
ÔÇ∑
ÔÇ∑ Analog-to-Digital Conversion (ADC):
o Sampling (rate, Nyquist theorem)
o Quantization (bit depth, quantization error)
ÔÇ∑
ÔÇ∑ Storage & playback: CD quality = 44.1 kHz, 16-bit, ~5.5MB/minute
ÔÇ∑
ÔÇ∑ Aliasing: Prevent with low-pass filters before sampling
3 | P a g e
Audio Features for ML:
1. Levels of Abstraction:
ÔÇ∑
ÔÇ∑ Low-level: ZCR, amplitude, RMS, raw waveform
ÔÇ∑
ÔÇ∑ Mid-level: Spectrograms, MFCCs
ÔÇ∑
ÔÇ∑ High-level: Melody, rhythm, key (musically interpretable)
2. Temporal Scope:
ÔÇ∑
ÔÇ∑ Instantaneous (e.g. RMS at a time slice)
ÔÇ∑
ÔÇ∑ Segmental (2‚Äì10 sec clips)
ÔÇ∑
ÔÇ∑ Global (whole track average features)
3. Domains:
ÔÇ∑
ÔÇ∑ Time-domain: Amplitude, ZCR
ÔÇ∑
ÔÇ∑ Frequency-domain: Spectral centroid, bandwidth, BER
ÔÇ∑
ÔÇ∑ Time-Frequency: Spectrograms, MFCCs, Mel spectrograms
Feature Extraction Techniques:
1.Time-Domain:
ÔÇ∑
ÔÇ∑ Amplitude envelope: Peak value per frame
ÔÇ∑
ÔÇ∑ Zero-Crossing Rate (ZCR): Frequency estimation and noisiness
ÔÇ∑
ÔÇ∑ Root Mean Square (RMS): Signal energy per frame
2.Frequency-Domain:
ÔÇ∑
ÔÇ∑ Fourier Transform (FT): Decomposes sound into sinusoids
ÔÇ∑
ÔÇ∑ Short-Time Fourier Transform (STFT): Tracks frequency changes over time
ÔÇ∑
ÔÇ∑ Spectrogram: STFT magnitude vs. time
ÔÇ∑
ÔÇ∑ Mel Spectrogram: STFT scaled to match human pitch perception
ÔÇ∑
ÔÇ∑ MFCCs: Compact cepstral features for speech/music classification
ÔÇ∑
ÔÇ∑ Spectral Features:
o Centroid: Brightness of sound
o Bandwidth: Spread of frequencies
o BER: Energy ratio in low vs high frequencies
4 | P a g e
Tools Used:
ÔÇ∑
ÔÇ∑ Librosa: Audio loading, STFT, MFCC, filtering
ÔÇ∑
ÔÇ∑ Matplotlib: Visualization of waveform, spectrograms
ÔÇ∑
ÔÇ∑ NumPy: Custom FFT, feature calculations
Hands-On Implementation Topics:
ÔÇ∑
ÔÇ∑ Visualizing waveforms & spectrograms
ÔÇ∑
ÔÇ∑ Extracting ZCR, RMS, amplitude envelope
ÔÇ∑
ÔÇ∑ Computing and interpreting:
o MFCCs and their derivatives (Œî, ŒîŒî)
o Spectral centroid and bandwidth
o Band energy ratios (BER)
ÔÇ∑
ÔÇ∑ Comparing features across genres (e.g. jazz vs rock vs classical)
Final Concepts:
ÔÇ∑
ÔÇ∑ Fourier Transform uses complex numbers (magnitude + phase)
ÔÇ∑
ÔÇ∑ Complex numbers are visualized using real + imaginary parts or polar coordinates
Conclusion:
This resource provides a strong foundation for audio ML tasks by blending theory, perceptual science, and coding. It is essential for building AI applications in speech recognition, music tagging, acoustic detection, and more.
We work in three domains:
ÔÇ∑
ÔÇ∑ Automatic speech recognition
ÔÇ∑
ÔÇ∑ Text to speech and vice Versa
ÔÇ∑
ÔÇ∑ PyTorch using NLP
5 | P a g e
1)Automatic speech recognition:
Tutorial,review and survey papers recent.
Publishers:IEEE, Wiley, springer, arxiv.
We thoroughly studied each research paper, understood the concepts, and analyzed the topics in depth. We also ensured a clear understanding of the research papers published by the above-mentioned authors in IEEE.
1) https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/iet-spr.2012.0151
2) https://ieeexplore.ieee.org/iel7/6287639/6514899/09536732.pdf
3) https://link.springer.com/content/pdf/10.1155/2010/926951.pdf
4) https://arxiv.org/pdf/1812.06864
Here is a summary of all four research papers you provided, focused on automatic speech recognition (ASR) techniques and applications:
1. Comparative Study of Automatic Speech Recognition Techniques
ÔÇ∑
ÔÇ∑ Type: Comparative Survey
ÔÇ∑
ÔÇ∑ Focus: Analyzes and compares different ASR techniques, especially hybrid models that combine multiple approaches.
ÔÇ∑
ÔÇ∑ Techniques Covered:
o Feature extraction: MFCC, LPC, Wavelet Transform
o Classifiers: HMM, ANN, SVM
ÔÇ∑
ÔÇ∑ Key Insight:
o Hybrid systems (e.g., MFCC + Wavelet + ANN) outperform single-method systems in accuracy and robustness.
ÔÇ∑
ÔÇ∑ Conclusion: Suggests future research should focus on combining complementary techniques to boost recognition rates.
6 | P a g e
2. Automatic Speech Recognition Using MFCC and DTW
ÔÇ∑
ÔÇ∑ Type: Experimental Research
ÔÇ∑
ÔÇ∑ Focus: Uses MFCC (Mel-Frequency Cepstral Coefficients) for feature extraction and DTW (Dynamic Time Warping) for speech pattern matching.
ÔÇ∑
ÔÇ∑ Methodology:
o Tested on isolated words recorded from 10 speakers.
o Compared DTW results to original templates.
ÔÇ∑
ÔÇ∑ Results:
o Accuracy: ~95% for speaker-dependent ASR using MFCC + DTW.
ÔÇ∑
ÔÇ∑ Conclusion: A simple and effective approach for small-vocabulary recognition tasks. Best suited for speaker-dependent systems.
3. ASR for Evaluation of Voice/Speech Disorders in Cancer Patients
ÔÇ∑
ÔÇ∑ Type: Clinical Application Study
ÔÇ∑
ÔÇ∑ Focus: Applies ASR to evaluate speech disorders in head and neck cancer patients post-treatment (e.g., after laryngectomy).
ÔÇ∑
ÔÇ∑ Methodology:
o Used ASR to assess intelligibility in recorded speech.
o Compared ASR results with human expert ratings.
ÔÇ∑
ÔÇ∑ Findings:
o High correlation between ASR output and expert judgment (r = -0.93).
o Patients' intelligibility ranged widely (8%‚Äì82% word recognition).
ÔÇ∑
ÔÇ∑ Conclusion:
o ASR provides an objective, fast, and non-invasive way to evaluate speech disorders.
7 | P a g e
4. Fully Convolutional Speech Recognition
ÔÇ∑
ÔÇ∑ Type: Deep Learning / Neural Architecture
ÔÇ∑
ÔÇ∑ Focus: Introduces an end-to-end ASR model using only convolutional layers, with no recurrent networks or hand-crafted features.
ÔÇ∑
ÔÇ∑ Innovations:
o Raw waveform input (no MFCCs or spectrograms).
o Convolutional encoder and convolutional language model.
o Training with AutoSeg and CTC loss functions.
ÔÇ∑
ÔÇ∑ Performance:
o Matches or outperforms RNN-based systems like DeepSpeech2 on WSJ and LibriSpeech datasets.
ÔÇ∑
ÔÇ∑ Conclusion:
o CNNs can fully replace RNNs in ASR pipelines, offering faster training and better parallelism without loss in accuracy.
8 | P a g e
2) Text to speech and vice Versa:
Tutorial,review and survey papers recent
These are the publishers
IEEE ,Wiley ,springer,arxiv,Taylor and francis
We thoroughly studied each research paper, understood the concepts, and analyzed the topics in depth. We also ensured a clear understanding of the research papers published by the above-mentioned authors in IEEE.
1) https://ieeexplore.ieee.org/iel7/6287639/6514899/10124769.pdf
2) https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog2601_4
3) https://link.springer.com/content/pdf/10.1007/BF00993821.pdf
4) https://arxiv.org/pdf/2006.04558
Here is a summary of all four research papers onText to speech and vice Versa you provided, focused on techniques and applications:
1. SAFE: Synthetic Audio Forensics Evaluation Challenge
ÔÇ∑
ÔÇ∑ Type: Overview and challenge design (2023)
ÔÇ∑
ÔÇ∑ Scope: Introduces the SAFE benchmark, designed to evaluate audio forensic tools in detecting synthetic speech and manipulated recordings.
ÔÇ∑
ÔÇ∑ Key Content:
o Defines evaluation metrics, protocols, datasets of both synthetic and real audio.
o Reviews existing forensic methods (e.g. spectral analysis, deep learning detectors).
o Details challenge rules, submission guidelines, and baseline performance benchmarks.
ÔÇ∑
ÔÇ∑ Significance: Offers a structured platform for comparing and improving methods in audio forensics.
9 | P a g e
2. Learning Words from Sights and Sounds: A Computational Model
ÔÇ∑
ÔÇ∑ Type: Computational cognitive modeling
ÔÇ∑
ÔÇ∑ Objective: Simulates how infants learn word‚Äìobject associations through multimodal sensory input (vision + audio).
ÔÇ∑
ÔÇ∑ Approach:
o Unsupervised learning‚Äîno labeled data.
o Models statistical regularities in synchronized audiovisual streams (images & associated words) and clusters representations into word‚Äìobject pairs.
ÔÇ∑
ÔÇ∑ Findings:
o Capable of developing a lexicon with reasonable alignment accuracy.
ÔÇ∑
ÔÇ∑ Impact: Supports theories of cross-modal bootstrapping in early language acquisition processes.
3. A Comparison of ID3 and Backpropagation for English Text-to-Speech Mapping
ÔÇ∑
ÔÇ∑ Type: Experimental ML comparison
ÔÇ∑
ÔÇ∑ Goal: Compares ID3 (decision-tree) vs backpropagation neural networks in mapping English text to phonemic and stress patterns.
ÔÇ∑
ÔÇ∑ Methodology:
o Employed distributed output coding (Sejnowski & Rosenberg style).
o Three hypotheses tested to explain performance differences.
ÔÇ∑
ÔÇ∑ Results:
o Backpropagation outperformed ID3 by several percentage points.
o Analysis indicated backprop‚Äôs ability to model richer statistical regularities was the key advantage.
o When ID3 was enhanced with statistical smoothing, its performance nearly matched BP.
ÔÇ∑
ÔÇ∑ Conclusion: Backprop excels in capturing nuanced dependencies, but simple enhancements can improve symbolic learners.
10 | P a g e
4. FastSpeech 2: Fast and High-Quality End-to-End Text-to-Speech
ÔÇ∑
ÔÇ∑ Type: Deep Learning ‚Äì Non-autoregressive TTS
ÔÇ∑
ÔÇ∑ Objective: Improve training efficiency and voice quality over the original FastSpeech model.
ÔÇ∑
ÔÇ∑ Key Innovations:
o Removes teacher-student distillation‚Äîuses ground-truth duration, pitch, and energy as conditioning signals.
o Introduces FastSpeech 2s, a fully end-to-end model directly synthesizing waveform.
ÔÇ∑
ÔÇ∑ Performance Gains:
o 3√ó faster training, simpler pipeline.
o Improves speech quality‚Äîsurpasses autoregressive TTS in MOS ratings.
o FastSpeech 2s offers even faster inference.
3) PyTorch using NLP:
We went through the PyTorch NLP playlist available on YouTube, which I have mentioned below.
1)https://youtube.com/playlist?list=PLqL-7eLmqd9V3faivSAST76YQClS44dSz&si=zRymMz6uu4J2vAE7
Note :We have been planning to work on this project, but we need some more time to complete it. So far, we have developed code that includes interaction only between the patient and the caretaker.
CODE:
# Install required libraries
!pip install pyttsx3 SpeechRecognition pyaudio --quiet
!pip install pipwin --quiet
!pipwin install pyaudio
import time
import pyttsx3
import speech_recognition as sr
11 | P a g e
import csv
from datetime import datetime, timedelta
import os
data = {
"patient_name": "Ravi",
"language": "en",
"medicines": [
{"name": "xyz", "time": "09:00"},
{"name": "xyz", "time": "14:00"},
{"name": "xyz", "time": "20:00"}
]
}
# For testing, update times to a few seconds ahead
now = datetime.now()
data["medicines"] = [
{"name": "xyz", "time": (now + timedelta(seconds=5)).strftime("%H:%M")},
{"name": "xyz", "time": (now + timedelta(seconds=10)).strftime("%H:%M")},
{"name": "xyz", "time": (now + timedelta(seconds=15)).strftime("%H:%M")},
]
tts_engine = pyttsx3.init()
tts_engine.setProperty('rate', 150)
def send_voice_reminder(message):
print(f"[TTS] {message}")
tts_engine.say(message)
tts_engine.runAndWait()
12 | P a g e
recognizer = sr.Recognizer()
def listen_patient_response():
try:
with sr.Microphone() as source:
print("üó£Ô∏è Listening for patient response...")
audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)
response = recognizer.recognize_google(audio)
print(f"‚úÖ Recognized: {response}")
return response.lower()
except Exception as e:
print(f"‚ö†Ô∏è STT error: {e}")
return "no response"
def save_to_csv(record, filename="medicine_log.csv"):
file_exists = os.path.isfile(filename)
with open(filename, mode='a', newline='') as file:
writer = csv.DictWriter(file, fieldnames=["timestamp", "medicine", "response"])
if not file_exists:
writer.writeheader()
writer.writerow(record)
print(f"üíæ Record saved: {record}")
def check_medicine_schedule():
now_time = datetime.now().strftime("%H:%M")
for med in data["medicines"]:
if med["time"] == now_time:
# Speak reminder
13 | P a g e
reminder = f"Namaste {data['patient_name']}, it's time to take your medicine {med['name']}."
send_voice_reminder(reminder)
# Listen to Ravi
response = listen_patient_response()
# Speak back Ravi's response
if response != "no response":
response_msg = f"Ravi, you said: {response}. Thank you!"
else:
response_msg = "Sorry Ravi, I could not understand you."
send_voice_reminder(response_msg)
record = {
"timestamp": datetime.now().isoformat(),
"medicine": med["name"],
"response": response
}
save_to_csv(record)
def run_scheduler_for_testing(duration_seconds=30):
print("[INFO] Starting reminder system in test mode...")
start_time = time.time()
while time.time() - start_time < duration_seconds:
check_medicine_schedule()
time.sleep(1)
run_scheduler_for_testing(30)
14 | P a g e
NOTE:
Currently, the code is running only in Jupyter Notebook. Our goal is to develop it further into an AI assistant. This assistant will enable interaction not only between the patient and the caretaker but also with family members or children living abroad. The system will connect them to the patient‚Äôs device, allowing the AI model to notify the patient‚Äîthrough speech or text‚Äîabout when to take their medicine. This will be highly effective in supporting the patient‚Äôs health and keeping family members informed and involved, even from a distance.
Introduction:
This project presents a voice-enabled medicine reminder system designed using Python, integrating machine learning concepts, text-to-speech (TTS) and speech-to-text (STT) technologies. The primary goal is to assist patients‚Äîespecially elderly or chronically ill individuals like the patient Ravi in this case‚Äîin adhering to their prescribed medication schedule. The system uses PyTorch (planned), Pyttsx3, SpeechRecognition, and pyaudio libraries, and can function as a basic AI assistant prototype. The assistant gives medicine reminders using TTS, listens for a response using STT, and logs every interaction for monitoring by caretakers or family members, including those living abroad.
System Design:
The system is designed with the following components:
1. Configuration Module
o Patient details (e.g., name, language)
o Medicine names and scheduled times (dynamically adjusted for testing)
2. Text-to-Speech Engine
o Uses pyttsx3 to generate voice alerts to remind the patient
3. Speech-to-Text Engine
o Uses SpeechRecognition and Google‚Äôs API to capture patient responses via microphone
4. Data Logging Module
o Patient interactions and responses are logged in a CSV file (medicine_log.csv) for future reference
5. Medicine Scheduler
o Continuously checks for scheduled medicine times and triggers the reminder accordingly
6. Testing Scheduler
15 | P a g e
o A test mode that simulates medicine reminders within short time intervals for demonstration.
Implementation:
The system has been implemented entirely in Python, running in a Jupyter Notebook environment. Below is a summary of the implementation details:
ÔÇ∑
ÔÇ∑ Libraries Used:
o pyttsx3: For TTS reminders
o SpeechRecognition + pyaudio: For capturing and converting patient speech to text
o csv, datetime, os, time: For data handling and scheduling
ÔÇ∑
ÔÇ∑ Voice Reminders: The assistant greets the patient with a personalized message and reminds them about their medicine.
ÔÇ∑
ÔÇ∑ Voice Input: The system listens to the patient‚Äôs voice and transcribes it using Google‚Äôs speech recognition engine.
ÔÇ∑
ÔÇ∑ CSV Logging: Every reminder and corresponding patient response is recorded with a timestamp for caregiver review.
ÔÇ∑
ÔÇ∑ Testing Mode: For demonstration purposes, medicine times are set a few seconds ahead and run for 30 seconds to simulate real-time functionality.
Conclusion:
This project demonstrates a foundational prototype of a personalized medicine reminder system that can later evolve into a smart AI assistant for remote patient care. Its ability to communicate with patients using voice and record responses makes it suitable for real-world use cases, particularly where the caretaker or family is located abroad. In the future, this system can be enhanced with deep learning models (using PyTorch), real-time remote notifications, mobile app integration, and multi-language support to improve scalability and accessibility.
References:
1. Pyttsx3 Documentation
2. SpeechRecognition Library
3. NPTEL PyTorch Playlist (as mentioned in your earlier inputs)
4. [IEEE Research Papers on AI in Healthcare] (as referred to in your study)